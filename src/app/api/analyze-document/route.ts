import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import { createRouteHandlerClient } from '@supabase/auth-helpers-nextjs'
import { cookies } from 'next/headers'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
})

// Saul Goodman-style Immigration Attorney Prompt
const DOCUMENT_ANALYSIS_PROMPT = `You are Sarah Chen, a badass immigration attorney with 30 years of experience. You're like Saul Goodman but for immigration law - quick, direct, and you get shit done.

Your style:
- NO long paragraphs or articles
- Straight to the point
- "No worries, here's what you gotta do..."
- Give specific form names and links when needed
- Act like you've seen everything in 30 years

When analyzing forms:
- Point out EXACTLY what's missing (dates, signatures, checkboxes)
- Tell them if something looks wrong
- Give step-by-step fixes
- Be encouraging but realistic

Example responses:
User: "My wife doesn't want to come to our green card interview"
You: "No worries! You can request to reschedule or ask for a waiver if she's sick. I'll walk you through it - first, call USCIS at..."

User: "Can you help me apply for citizenship?"
You: "You need Form N-400. Here's the link: uscis.gov/n-400. I can help you step by step. First question - how long have you been a permanent resident?"

CRITICAL: Base everything on the ACTUAL form content I give you. Be specific about what fields are filled vs empty.`

// Interface for better typing
interface PDFField {
  name: string;
  value: string;
  type: string;
}

// Super aggressive PDF parsing function
async function parsePDF(buffer: Buffer): Promise<string> {
  console.log('ðŸ” Starting AGGRESSIVE PDF parsing...')
  let extractedContent = ''
  
  try {
    // Method 1: Standard pdf-parse with detailed logging
    console.log('ðŸ“„ Method 1: Standard pdf-parse')
    try {
      const pdfParse = (await import('pdf-parse')).default
      const pdfData = await pdfParse(buffer, {
        max: 0 // Disable page limit to avoid test file issues
      })

      console.log(`PDF Info - Pages: ${pdfData.numpages}, Text Length: ${pdfData.text.length}`)
      console.log(`First 500 chars of text: "${pdfData.text.substring(0, 500)}"`)
      
      if (pdfData.text.trim().length > 20) {
        extractedContent += `PDF-PARSE CONTENT:\n${pdfData.text}\n\n`
      }
    } catch (pdfParseError) {
      console.log('pdf-parse failed:', pdfParseError)
      extractedContent += `PDF-PARSE FAILED: ${pdfParseError}\n\n`
    }

    // Method 2: pdf-lib form field extraction with extensive logging
    console.log('ðŸ“„ Method 2: pdf-lib form field extraction')
    try {
      const { PDFDocument } = await import('pdf-lib')
      const pdfDoc = await PDFDocument.load(buffer)
      
      // Get document info
      const pageCount = pdfDoc.getPageCount()
      const title = pdfDoc.getTitle()
      const creator = pdfDoc.getCreator()
      
      console.log(`PDF Metadata - Pages: ${pageCount}, Title: ${title}, Creator: ${creator}`)
      
      const form = pdfDoc.getForm()
      const fields = form.getFields()
      
      console.log(`Found ${fields.length} form fields`)
      
      let formFieldsText = ''
      const processedFields: PDFField[] = []
      
      fields.forEach((field, index) => {
        try {
          const fieldName = field.getName()
          const fieldType = field.constructor.name
          
          console.log(`Field ${index}: ${fieldName} (${fieldType})`)
          
          let fieldValue = ''
          
          // Handle different field types with proper error handling
          if (fieldType.includes('TextField')) {
            try {
              const textField = field as unknown as { getText(): string }
              fieldValue = textField.getText ? textField.getText() : 'NO_TEXT_METHOD'
            } catch {
              fieldValue = 'TEXT_FIELD_ERROR'
            }
          } else if (fieldType.includes('CheckBox')) {
            try {
              const checkField = field as unknown as { isChecked(): boolean }
              fieldValue = checkField.isChecked ? (checkField.isChecked() ? 'CHECKED' : 'UNCHECKED') : 'NO_CHECKED_METHOD'
            } catch {
              fieldValue = 'CHECKBOX_ERROR'
            }
          } else if (fieldType.includes('Dropdown')) {
            try {
              const dropdownField = field as unknown as { getSelected(): string[] | undefined }
              const selected = dropdownField.getSelected ? dropdownField.getSelected() : null
              fieldValue = selected ? JSON.stringify(selected) : 'NO_SELECTION'
            } catch {
              fieldValue = 'DROPDOWN_ERROR'
            }
          } else {
            fieldValue = `UNKNOWN_TYPE_${fieldType}`
          }
          
          processedFields.push({ name: fieldName, value: fieldValue, type: fieldType })
          formFieldsText += `${fieldName}: ${fieldValue}\n`
          console.log(`  Value: ${fieldValue}`)
          
        } catch (fieldError) {
          console.log(`Field ${index} processing error:`, fieldError)
          formFieldsText += `Field_${index}: PROCESSING_ERROR\n`
        }
      })
      
      if (formFieldsText.trim()) {
        extractedContent += `FORM FIELDS (${fields.length} total):\n${formFieldsText}\n\n`
      }
      
    } catch (pdfLibError) {
      console.log('pdf-lib error:', pdfLibError)
      extractedContent += `PDF-LIB ERROR: ${pdfLibError}\n\n`
    }

    // Method 3: Raw buffer analysis with multiple encodings
    console.log('ðŸ“„ Method 3: Raw buffer analysis')
    
    // Try different encodings
    const encodings: BufferEncoding[] = ['utf8', 'ascii', 'latin1', 'binary']
    
    encodings.forEach(encoding => {
      try {
        const rawText = buffer.toString(encoding)
        console.log(`${encoding} encoding - Length: ${rawText.length}`)
        
        // Look for text patterns
        const patterns = [
          /\/T\s*\(([^)]+)\)/g,     // Text field values
          /\/V\s*\(([^)]+)\)/g,     // Field values  
          /\/DA\s*\(([^)]+)\)/g,    // Default appearance
          /BT\s+([^E]+)ET/g,        // Text objects
          /\(([^)]{3,50})\)/g,      // Any text in parentheses
          /\/F\d+\s+(\w+)/g,        // Font references
          /stream\s*\n([^e]+)endstream/g, // Stream content
        ]
        
        let patternText = ''
        patterns.forEach((pattern, i) => {
          const matches = rawText.match(pattern)
          if (matches) {
            console.log(`Pattern ${i} found ${matches.length} matches in ${encoding}`)
            matches.slice(0, 10).forEach(match => { // Limit to first 10 matches
              const cleaned = match.replace(/[\/\(\)BTET]/g, '').trim()
              if (cleaned.length > 2 && cleaned.match(/[a-zA-Z]/)) {
                patternText += cleaned + ' '
              }
            })
          }
        })
        
        if (patternText.trim().length > 10) {
          extractedContent += `${encoding.toUpperCase()} PATTERNS:\n${patternText.trim()}\n\n`
        }
        
      } catch (encodingError) {
        console.log(`${encoding} encoding failed:`, encodingError)
      }
    })

    // Method 4: Hex dump analysis
    console.log('ðŸ“„ Method 4: Hex analysis')
    try {
      const hexString = buffer.toString('hex')
      console.log(`Hex length: ${hexString.length}`)
      
      // Look for common PDF strings in hex
      const commonStrings = ['Name', 'Type', 'Kids', 'Parent', 'Page', 'Text', 'Font', 'obj', 'endobj']
      const hexFindings: string[] = []
      
      commonStrings.forEach(str => {
        const hexPattern = Buffer.from(str, 'utf8').toString('hex')
        if (hexString.includes(hexPattern)) {
          hexFindings.push(str)
        }
      })
      
      if (hexFindings.length > 0) {
        extractedContent += `HEX ANALYSIS - Found: ${hexFindings.join(', ')}\n\n`
      }
      
    } catch (hexError) {
      console.log('Hex analysis failed:', hexError)
    }

    // Method 5: Chunk analysis
    console.log('ðŸ“„ Method 5: Chunk analysis')
    try {
      const chunkSize = 1000
      const chunks = []
      
      for (let i = 0; i < buffer.length; i += chunkSize) {
        const chunk = buffer.slice(i, i + chunkSize)
        const chunkText = chunk.toString('utf8').replace(/[^\x20-\x7E]/g, '')
        
        if (chunkText.length > 50) {
          chunks.push(`Chunk ${Math.floor(i/chunkSize)}: ${chunkText.substring(0, 200)}`)
        }
      }
      
      if (chunks.length > 0) {
        extractedContent += `CHUNKS FOUND:\n${chunks.slice(0, 5).join('\n')}\n\n`
      }
      
    } catch (chunkError) {
      console.log('Chunk analysis failed:', chunkError)
    }

    console.log(`ðŸ“„ Total extracted content length: ${extractedContent.length}`)
    console.log(`ðŸ“„ First 1000 chars: "${extractedContent.substring(0, 1000)}"`)

    return extractedContent.trim()

  } catch (error) {
    console.error('âŒ Complete PDF parsing failure:', error)
    return `PARSING_ERROR: ${error}`
  }
}

export async function POST(request: NextRequest) {
  console.log('ðŸ” Document analysis API called')

  try {
    const formData = await request.formData()
    const file = formData.get('file') as File
    const chatSessionId = formData.get('chatSessionId') as string

    console.log('ðŸ“ File received:', file?.name, 'Size:', file?.size, 'Type:', file?.type)

    if (!file) {
      return NextResponse.json({ error: 'No file provided' }, { status: 400 })
    }

    // Get authenticated user
    const cookieStore = cookies()
    const supabase = createRouteHandlerClient({ cookies: () => cookieStore })
    const { data: { user }, error: authError } = await supabase.auth.getUser()

    if (authError || !user) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    // Check if user has premium subscription
    const { data: userData, error: userError } = await supabase
      .from('users')
      .select('subscription_status')
      .eq('id', user.id)
      .single()

    if (userError || userData.subscription_status !== 'premium') {
      return NextResponse.json({
        error: 'Document analysis is a premium feature. Please upgrade to access this functionality.'
      }, { status: 403 })
    }

    console.log('âœ… Premium user verified, analyzing document...')

    // Convert file to buffer for storage and processing
    const arrayBuffer = await file.arrayBuffer()
    const buffer = Buffer.from(arrayBuffer)

    // Upload file to Supabase storage (optional - continue if fails)
    const fileName = `${Date.now()}_${file.name}`
    const storagePath = `uploads/${user.id}/${fileName}`

    console.log('ðŸ“¤ Uploading file to Supabase storage...')
    try {
      const { data: uploadData, error: uploadError } = await supabase.storage
        .from('documents')
        .upload(storagePath, buffer, {
          contentType: file.type,
          upsert: true // Allow overwrite to avoid conflicts
        })

      if (uploadError) {
        console.log('âŒ Storage upload error (continuing anyway):', uploadError)
      } else {
        console.log('âœ… File uploaded to storage:', uploadData.path)
      }
    } catch (storageError) {
      console.log('âŒ Storage upload failed (continuing anyway):', storageError)
    }

    let analysisResult: string

    try {
      if (file.type.startsWith('image/')) {
        console.log('ðŸ–¼ï¸ Processing image with AI Vision...')

        const fileBase64 = Buffer.from(arrayBuffer).toString('base64')

        const response = await openai.chat.completions.create({
          model: 'gpt-4o',
          messages: [
            {
              role: 'system',
              content: DOCUMENT_ANALYSIS_PROMPT
            },
            {
              role: 'user',
              content: [
                {
                  type: 'text',
                  text: `Analyze this immigration form image "${file.name}". Tell me exactly what's filled out, what's missing, and what needs to be fixed. Be direct and specific like you've been doing this for 30 years.`
                },
                {
                  type: 'image_url',
                  image_url: {
                    url: `data:${file.type};base64,${fileBase64}`,
                    detail: 'high'
                  }
                }
              ]
            }
          ],
          max_tokens: 1000,
          temperature: 0.3
        })

        analysisResult = response.choices[0].message.content || 'Unable to analyze document'
        console.log('âœ… Image analysis completed')

      } else if (file.type === 'application/pdf') {
        console.log('ðŸ“„ Processing PDF file - trying image conversion...')

        // For protected/encrypted PDFs, convert to image and use vision
        try {
          console.log('ðŸ–¼ï¸ Converting PDF to image for vision analysis...')
          
          // Use pdf2pic to convert first page to image
          const pdf2pic = await import('pdf2pic')
          
          const convertOptions = {
            density: 200,           // High resolution
            saveFilename: "page",
            savePath: "/tmp/",
            format: "png",
            width: 2000,           // High width for clarity
            height: 2600           // Standard form height
          }
          
          const convert = pdf2pic.fromBuffer(buffer, convertOptions)
          const pageImage = await convert(1, { responseType: "buffer" })
          
          if (pageImage && pageImage.buffer) {
            console.log('âœ… PDF converted to image, analyzing with vision...')
            
            const imageBase64 = pageImage.buffer.toString('base64')
            
            const response = await openai.chat.completions.create({
              model: 'gpt-4o',
              messages: [
                {
                  role: 'system',
                  content: DOCUMENT_ANALYSIS_PROMPT
                },
                {
                  role: 'user',
                  content: [
                    {
                      type: 'text',
                      text: `Analyze this immigration form PDF (converted to image) "${file.name}". Tell me exactly what's filled out, what's missing, and what needs to be fixed. Be direct and specific like you've been doing this for 30 years.`
                    },
                    {
                      type: 'image_url',
                      image_url: {
                        url: `data:image/png;base64,${imageBase64}`,
                        detail: 'high'
                      }
                    }
                  ]
                }
              ],
              max_tokens: 1000,
              temperature: 0.3
            })

            analysisResult = response.choices[0].message.content || 'Unable to analyze document'
            console.log('âœ… PDF->Image analysis completed')
          } else {
            throw new Error('PDF conversion failed')
          }
          
        } catch (conversionError) {
          console.log('PDF->Image conversion failed, trying text extraction...', conversionError)
          
          // Fallback to text extraction
          const extractedText = await parsePDF(buffer)
          console.log(`ðŸ“„ Final extracted text: ${extractedText.length} characters`)

          if (extractedText.trim().length < 100) {
            analysisResult = `Your PDF "${file.name}" appears to be encrypted or protected (common with USCIS forms).

**Quick Fix:**
1. **Take screenshots** of each page as JPG/PNG
2. **Upload the images** - I can read them perfectly!
3. Or tell me what form this is (I-130, I-485, etc.) and I'll guide you step by step

What immigration form are you working on?`
          } else {
            // Analyze extracted text
            const response = await openai.chat.completions.create({
              model: 'gpt-4',
              messages: [
                {
                  role: 'system',
                  content: DOCUMENT_ANALYSIS_PROMPT
                },
                {
                  role: 'user',
                  content: `Here's what I extracted from "${file.name}":

${extractedText.slice(0, 7000)}

Analyze this immigration form. Tell me exactly what's filled out, what's missing, and what needs to be fixed.`
                }
              ],
              max_tokens: 1000,
              temperature: 0.3
            })

            analysisResult = response.choices[0].message.content || 'Unable to analyze document'
            console.log('âœ… Text-based PDF analysis completed')
          }
        }

      } else {
        return NextResponse.json({ error: 'Upload PDF or image files only.' }, { status: 400 })
      }

    } catch (aiError) {
      console.error('AI Analysis Error:', aiError)
      analysisResult = `Technical hiccup analyzing your document, but no worries!

Tell me:
- What immigration form are you working on?
- What specific questions do you have?
- What sections are giving you trouble?

I can help you step by step even without the file analysis.`
    }

    // Save to database
    try {
      await supabase.from('uploaded_files').insert({
        user_id: user.id,
        chat_session_id: chatSessionId,
        file_name: file.name,
        file_size: file.size,
        file_type: file.type,
        storage_path: storagePath,
        analysis_result: { analysis: analysisResult }
      })

      await supabase.from('messages').insert({
        chat_session_id: chatSessionId,
        user_id: user.id,
        content: analysisResult,
        role: 'assistant',
        ai_provider: 'openai'
      })

      console.log('ðŸ’¾ Analysis saved successfully')
    } catch (dbError) {
      console.log('âŒ Database save error:', dbError)
    }

    return NextResponse.json({
      analysis: analysisResult,
      success: true
    })

  } catch (error) {
    console.error('ðŸ’¥ Document Analysis Error:', error)
    return NextResponse.json({
      error: 'Something went wrong analyzing your document. Try again.',
      details: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 })
  }
}